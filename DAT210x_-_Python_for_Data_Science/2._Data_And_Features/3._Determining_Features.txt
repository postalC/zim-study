Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.4
Creation-Date: 2016-10-15T22:57:28+08:00

====== 3. Determining Features ======
Created Saturday 15 October 2016

===== Determining Features =====

With a challenge in mind you want to solve, you know that you need to collect a lot of samples along with features that describe them. This is the prerequisite for machine learning. You now also know these features can be continuous numeric values, or they can be categorical. But how exactly should you go about choosing features? And what's more important to focus onâ€”adding additional features or collecting more samples?

These are reasonable questions everyone has when they start amassing the data they need to solve an issue. The answer is, it depends. Just as in the example of Angie & Craig's lists mentioned in the Machine Learning section, your own intuition about the problem being tackled should really be the driving force behind what data you collect. The only unbreakable rule is that you need to ensure you collect as many features and samples as you possibly can.

If you ever become unsure which one of the two you should focus on more, concentrate on collecting more samples. A least during collection, try to make sure you have more samples than features because some machine learning algorithms won't work well if that isn't the case. This is also known as the curse of dimensionality. At its core, many algorithms are implemented as matrix operations, and without a greater than or equal number of samples to features, a fully-formed system of independent equations cannot be made. You can always create more features based off of your existing ones. But creating pseudo-samples, while not impossible, might be a bit more difficult.

{{../images/Section2.jpg}}
